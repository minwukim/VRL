[2025-03-12 13:12:46,782] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0312 13:13:15.533000 8723 site-packages/torch/distributed/run.py:793] 
W0312 13:13:15.533000 8723 site-packages/torch/distributed/run.py:793] *****************************************
W0312 13:13:15.533000 8723 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0312 13:13:15.533000 8723 site-packages/torch/distributed/run.py:793] *****************************************
[2025-03-12 13:14:54,873] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-12 13:14:56,873] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO 03-12 13:15:50 __init__.py:190] Automatically detected platform cuda.
INFO 03-12 13:15:51 __init__.py:190] Automatically detected platform cuda.
MyArguments(model_name='Qwen/Qwen2.5-3B', output_dir='outputs/qwen2.5-3b-grpo-full', run_name='qwen3b-full', learning_rate=1e-06, beta=0.001, adam_beta1=0.9, adam_beta2=0.99, weight_decay=0.1, warmup_steps=25, lr_scheduler_type='constant_with_warmup', logging_steps=1, bf16=True, bf16_full_eval=True, per_device_train_batch_size=4, gradient_accumulation_steps=4, gradient_checkpointing=True, num_generations=8, max_prompt_length=256, max_completion_length=4096, num_train_epochs=1, save_steps=50, max_grad_norm=0.1, report_to='wandb', use_vllm=True, vllm_max_model_len=5000, max_steps=400, log_completions=True, evaluation_strategy='steps', eval_steps=50, eval_on_start=False, resume_from_checkpoint=True)
MyArguments(model_name='Qwen/Qwen2.5-3B', output_dir='outputs/qwen2.5-3b-grpo-full', run_name='qwen3b-full', learning_rate=1e-06, beta=0.001, adam_beta1=0.9, adam_beta2=0.99, weight_decay=0.1, warmup_steps=25, lr_scheduler_type='constant_with_warmup', logging_steps=1, bf16=True, bf16_full_eval=True, per_device_train_batch_size=4, gradient_accumulation_steps=4, gradient_checkpointing=True, num_generations=8, max_prompt_length=256, max_completion_length=4096, num_train_epochs=1, save_steps=50, max_grad_norm=0.1, report_to='wandb', use_vllm=True, vllm_max_model_len=5000, max_steps=400, log_completions=True, evaluation_strategy='steps', eval_steps=50, eval_on_start=False, resume_from_checkpoint=True)
[2025-03-12 13:15:59,321] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-12 13:16:01,339] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-03-12 13:16:01,340] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-12 13:16:01,841] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-03-12 13:16:03,673] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
[2025-03-12 13:16:04,983] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 435, num_elems = 3.40B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.08it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.05it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.25it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.27it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.23it/s]
[2025-03-12 13:16:06,883] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-03-12 13:16:06,896] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-03-12 13:16:07,085] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 870, num_elems = 6.79B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.07it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.01it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.26it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.23it/s]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.18it/s]
INFO 03-12 13:18:09 config.py:542] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.
INFO 03-12 13:18:09 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-3B', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:2, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 03-12 13:18:16 cuda.py:230] Using Flash Attention backend.
INFO 03-12 13:18:16 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-3B...
INFO 03-12 13:18:17 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.76it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.84it/s]

INFO 03-12 13:18:18 model_runner.py:1115] Loading model weights took 0.0000 GB
INFO 03-12 13:18:19 worker.py:267] Memory profiling takes 0.53 seconds
INFO 03-12 13:18:19 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.90) = 71.29GiB
INFO 03-12 13:18:19 worker.py:267] model weights take 0.00GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.00GiB; the rest of the memory reserved for KV Cache is 71.29GiB.
INFO 03-12 13:18:19 executor_base.py:110] # CUDA blocks: 129775, # CPU blocks: 7281
INFO 03-12 13:18:19 executor_base.py:115] Maximum concurrency for 5000 tokens per request: 415.28x
INFO 03-12 13:18:22 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|‚ñé         | 1/35 [00:00<00:12,  2.71it/s]Capturing CUDA graph shapes:   6%|‚ñå         | 2/35 [00:00<00:10,  3.00it/s]Capturing CUDA graph shapes:   9%|‚ñä         | 3/35 [00:00<00:10,  3.11it/s]Capturing CUDA graph shapes:  11%|‚ñà‚ñè        | 4/35 [00:01<00:10,  3.05it/s]Capturing CUDA graph shapes:  14%|‚ñà‚ñç        | 5/35 [00:01<00:12,  2.41it/s]Capturing CUDA graph shapes:  17%|‚ñà‚ñã        | 6/35 [00:02<00:11,  2.48it/s]Capturing CUDA graph shapes:  20%|‚ñà‚ñà        | 7/35 [00:02<00:10,  2.58it/s]Capturing CUDA graph shapes:  23%|‚ñà‚ñà‚ñé       | 8/35 [00:02<00:10,  2.68it/s]Capturing CUDA graph shapes:  26%|‚ñà‚ñà‚ñå       | 9/35 [00:03<00:09,  2.74it/s]Capturing CUDA graph shapes:  29%|‚ñà‚ñà‚ñä       | 10/35 [00:03<00:09,  2.76it/s]Capturing CUDA graph shapes:  31%|‚ñà‚ñà‚ñà‚ñè      | 11/35 [00:04<00:08,  2.78it/s]Capturing CUDA graph shapes:  34%|‚ñà‚ñà‚ñà‚ñç      | 12/35 [00:04<00:08,  2.80it/s]Capturing CUDA graph shapes:  37%|‚ñà‚ñà‚ñà‚ñã      | 13/35 [00:04<00:07,  2.80it/s]Capturing CUDA graph shapes:  40%|‚ñà‚ñà‚ñà‚ñà      | 14/35 [00:05<00:07,  2.73it/s]Capturing CUDA graph shapes:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 15/35 [00:05<00:07,  2.72it/s]Capturing CUDA graph shapes:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 16/35 [00:05<00:06,  2.76it/s]Capturing CUDA graph shapes:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 17/35 [00:06<00:06,  2.77it/s]Capturing CUDA graph shapes:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 18/35 [00:06<00:06,  2.79it/s]Capturing CUDA graph shapes:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 19/35 [00:06<00:05,  2.82it/s]Capturing CUDA graph shapes:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 20/35 [00:07<00:05,  2.80it/s]Capturing CUDA graph shapes:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/35 [00:07<00:05,  2.76it/s]Capturing CUDA graph shapes:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 22/35 [00:07<00:04,  2.79it/s]Capturing CUDA graph shapes:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 23/35 [00:08<00:04,  2.77it/s]Capturing CUDA graph shapes:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 24/35 [00:08<00:03,  2.75it/s]Capturing CUDA graph shapes:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 25/35 [00:09<00:03,  2.70it/s]Capturing CUDA graph shapes:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 26/35 [00:09<00:03,  2.75it/s]Capturing CUDA graph shapes:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 27/35 [00:09<00:02,  2.79it/s]Capturing CUDA graph shapes:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 28/35 [00:10<00:02,  2.88it/s]Capturing CUDA graph shapes:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 29/35 [00:10<00:02,  2.88it/s]Capturing CUDA graph shapes:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 30/35 [00:10<00:01,  2.59it/s]Capturing CUDA graph shapes:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 31/35 [00:11<00:01,  2.50it/s]Capturing CUDA graph shapes:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 32/35 [00:11<00:01,  2.49it/s]Capturing CUDA graph shapes:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 33/35 [00:12<00:00,  2.56it/s]Capturing CUDA graph shapes:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 34/35 [00:12<00:00,  2.70it/s]Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:12<00:00,  2.80it/s]Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:12<00:00,  2.73it/s]
INFO 03-12 13:18:35 model_runner.py:1562] Graph capturing finished in 13 secs, took 0.00 GiB
INFO 03-12 13:18:35 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 16.21 seconds
[2025-03-12 13:18:38,554] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed info: version=0.15.3, git-hash=unknown, git-branch=unknown
[2025-03-12 13:18:38,555] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-03-12 13:18:38,556] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2
[2025-03-12 13:18:38,575] [INFO] [logging.py:129:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-12 13:18:38,578] [INFO] [logging.py:129:log_dist] [Rank 0] Creating ZeRO Offload
[2025-03-12 13:18:39,135] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-03-12 13:18:39,137] [INFO] [utils.py:782:see_memory_usage] MA 5.75 GB         Max_MA 5.75 GB         CA 5.91 GB         Max_CA 6 GB 
[2025-03-12 13:18:39,138] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 89.78 GB, percent = 4.5%
Parameter Offload: Total persistent parameters: 241664 in 181 params
[2025-03-12 13:18:39,711] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-03-12 13:18:39,713] [INFO] [utils.py:782:see_memory_usage] MA 5.75 GB         Max_MA 5.75 GB         CA 5.91 GB         Max_CA 6 GB 
[2025-03-12 13:18:39,717] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 89.73 GB, percent = 4.5%
[2025-03-12 13:18:39,719] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2025-03-12 13:18:39,719] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-03-12 13:18:39,720] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-12 13:18:39,721] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2025-03-12 13:18:39,721] [INFO] [config.py:1003:print]   amp_params ................... False
[2025-03-12 13:18:39,722] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-12 13:18:39,723] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2025-03-12 13:18:39,724] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2025-03-12 13:18:39,724] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2025-03-12 13:18:39,725] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2025-03-12 13:18:39,725] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2025-03-12 13:18:39,726] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x77d6cffb8400>
[2025-03-12 13:18:39,726] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2025-03-12 13:18:39,727] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-12 13:18:39,728] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2025-03-12 13:18:39,729] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2025-03-12 13:18:39,729] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-12 13:18:39,730] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2025-03-12 13:18:39,731] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2025-03-12 13:18:39,731] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2025-03-12 13:18:39,732] [INFO] [config.py:1003:print]   dump_state ................... False
[2025-03-12 13:18:39,733] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2025-03-12 13:18:39,734] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2025-03-12 13:18:39,735] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-12 13:18:39,735] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-12 13:18:39,736] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2025-03-12 13:18:39,736] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2025-03-12 13:18:39,737] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2025-03-12 13:18:39,737] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2025-03-12 13:18:39,738] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2025-03-12 13:18:39,739] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2025-03-12 13:18:39,739] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-03-12 13:18:39,740] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2025-03-12 13:18:39,740] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2025-03-12 13:18:39,741] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2025-03-12 13:18:39,742] [INFO] [config.py:1003:print]   global_rank .................. 0
[2025-03-12 13:18:39,743] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2025-03-12 13:18:39,743] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4
[2025-03-12 13:18:39,744] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.1
[2025-03-12 13:18:39,745] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2025-03-12 13:18:39,746] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2025-03-12 13:18:39,746] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-12 13:18:39,747] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2025-03-12 13:18:39,747] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2025-03-12 13:18:39,748] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2025-03-12 13:18:39,749] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2025-03-12 13:18:39,749] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2025-03-12 13:18:39,750] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2025-03-12 13:18:39,751] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-12 13:18:39,752] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-03-12 13:18:39,752] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2025-03-12 13:18:39,753] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2025-03-12 13:18:39,753] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2025-03-12 13:18:39,754] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-12 13:18:39,755] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2025-03-12 13:18:39,756] [INFO] [config.py:1003:print]   pld_params ................... False
[2025-03-12 13:18:39,756] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2025-03-12 13:18:39,757] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2025-03-12 13:18:39,758] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2025-03-12 13:18:39,759] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-12 13:18:39,759] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2025-03-12 13:18:39,760] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2025-03-12 13:18:39,761] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2025-03-12 13:18:39,761] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2025-03-12 13:18:39,762] [INFO] [config.py:1003:print]   train_batch_size ............. 32
[2025-03-12 13:18:39,763] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  4
[2025-03-12 13:18:39,764] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2025-03-12 13:18:39,764] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2025-03-12 13:18:39,765] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2025-03-12 13:18:39,766] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2025-03-12 13:18:39,767] [INFO] [config.py:1003:print]   world_size ................... 2
[2025-03-12 13:18:39,767] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2025-03-12 13:18:39,768] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-03-12 13:18:39,769] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2025-03-12 13:18:39,770] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-12 13:18:39,771] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2025-03-12 13:18:39,772] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 4, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 0.1, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_optimization.reduce_bucket_size": 4.194304e+06, 
    "zero_optimization.stage3_param_persistence_threshold": 2.048000e+04, 
    "zero_optimization.stage3_prefetch_bucket_size": 3.774874e+06
}
Parameter Offload: Total persistent parameters: 241664 in 181 params
/home/VRL/env/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  partition = torch.load(path, map_location=map_location)
/home/VRL/env/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  partition = torch.load(path, map_location=map_location)
/home/VRL/env/lib/python3.10/site-packages/transformers/trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ss13750 (ss13750-new-york-university-abu-dhabi) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/VRL/wandb/run-20250312_132007-y6s5bf02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run qwen3b-full
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ss13750-new-york-university-abu-dhabi/huggingface
wandb: üöÄ View run at https://wandb.ai/ss13750-new-york-university-abu-dhabi/huggingface/runs/y6s5bf02
  0%|          | 0/400 [00:00<?, ?it/s]/home/VRL/env/lib/python3.10/site-packages/transformers/trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint_rng_state = torch.load(rng_file)
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 202/400 [01:10<01:08,  2.88it/s]                                                 {'loss': 0.0, 'grad_norm': 0.5484812474208625, 'learning_rate': 1e-06, 'rewards/reward_correct': 0.46875, 'rewards/reward_correct_and_format': 0.0, 'reward': 0.46875, 'reward_std': 0.2041158601641655, 'completion_length': 1147.03125, 'kl': 0.01116943359375, 'epoch': 0.11}
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 202/400 [01:10<01:08,  2.88it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [01:27<01:31,  2.15it/s]                                                 {'loss': 0.0, 'grad_norm': 0.3973453092322834, 'learning_rate': 1e-06, 'rewards/reward_correct': 0.875, 'rewards/reward_correct_and_format': 0.0, 'reward': 0.875, 'reward_std': 0.13363061845302582, 'completion_length': 394.875, 'kl': 0.0300140380859375, 'epoch': 0.11}
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 203/400 [01:27<01:31,  2.15it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 204/400 [02:08<02:49,  1.15it/s]                                                 {'loss': 0.0, 'grad_norm': 0.5006448048752732, 'learning_rate': 1e-06, 'rewards/reward_correct': 0.75, 'rewards/reward_correct_and_format': 0.0, 'reward': 0.75, 'reward_std': 0.1767766922712326, 'completion_length': 879.90625, 'kl': 0.02854156494140625, 'epoch': 0.11}
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 204/400 [02:08<02:49,  1.15it/s]