# model_name: "Qwen/Qwen2.5-0.5B"
# output_dir: "qwen2.5-0.5b-grpo-switch-type1"
# run_name: "qwen0.5b-switch-type1"
# resume_from_checkpoint: False
# # checkpoint_path: "outputs/qwen2.5-3b-grpo-large/checkpoint-125"
# learning_rate: 1e-6
# beta: 0.001
# adam_beta1: 0.9
# adam_beta2: 0.99
# weight_decay: 0.1
# warmup_steps: 25
# lr_scheduler_type: constant_with_warmup
# logging_steps: 1
# bf16: True
# bf16_full_eval: True
# per_device_train_batch_size: 4
# gradient_accumulation_steps: 5
# gradient_checkpointing: True
# num_generations: 8
# max_prompt_length: 2560
# max_completion_length: 2048
# num_train_epochs: 1
# save_steps: 25
# max_grad_norm: 0.1
# report_to: wandb
# use_vllm: True
# vllm_max_model_len: 6500
# max_steps: 500  
# log_completions: True
# evaluation_strategy: steps
# eval_steps: 50
# eval_on_start: False 


model_name: "Qwen/Qwen2.5-Math-1.5B-Instruct"
output_dir: "qwen2.5-1.5B-MATH-it-vanilla-GRPO-scaleTrue"
run_name: "qwen2.5-1.5B-MATH-it-vanillaGRPO"
scale_rewards: False
resume_from_checkpoint: False
# checkpoint_path: "outputs/qwen2.5-3b-grpo-large/checkpoint-125"
learning_rate: 1e-6
beta: 0.0001
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
warmup_steps: 20
lr_scheduler_type: constant_with_warmup
logging_steps: 1
bf16: True
bf16_full_eval: True
per_device_train_batch_size: 8
gradient_accumulation_steps: 5
gradient_checkpointing: True
num_generations: 8
max_prompt_length: 2000
max_completion_length: 1800
num_train_epochs: 1
save_steps: 50
max_grad_norm: 0.1
report_to: wandb
use_vllm: True
vllm_max_model_len: 4000
max_steps: 500  
log_completions: True
evaluation_strategy: steps
eval_steps: 50
eval_on_start: False 