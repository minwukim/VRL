model_name: "Qwen/Qwen2.5-3B"
#model_name: "../outputs/qwen2.5-3b-sft-pro/checkpoint-1092/" 
output_dir: "../outputs/qwen2.5-3b-grpo-math-small"
run_name: "qwen3b-grpo-math100"
resume_from_checkpoint: True 
checkpoint_path: "../outputs/qwen2.5-3b-grpo-math-small/checkpoint-125"
learning_rate: 1e-6
beta: 0
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
warmup_steps: 25
lr_scheduler_type: constant_with_warmup
logging_steps: 1
bf16: True
bf16_full_eval: True
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
gradient_checkpointing: True
num_generations: 10
max_prompt_length: 256
max_completion_length: 12000 
num_train_epochs: 10
save_steps: 50
max_grad_norm: 0.1
report_to: wandb
use_vllm: True
vllm_max_model_len: 8192 
max_steps: -1
log_completions: True
evaluation_strategy: no
eval_steps: 1 
eval_on_start: False 
