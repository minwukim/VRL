nohup: ignoring input
INFO 05-08 12:59:13 __init__.py:190] Automatically detected platform cuda.

>>> Starting evaluations on OOD_BASE — 21 questions x 128 trials
INFO 05-08 13:01:42 config.py:542] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 05-08 13:01:42 config.py:1401] Defaulting to use mp for distributed inference
INFO 05-08 13:01:42 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='./outputs/qwen2.5-3b-sft-pro/checkpoint-1092', speculative_config=None, tokenizer='./outputs/qwen2.5-3b-sft-pro/checkpoint-1092', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./outputs/qwen2.5-3b-sft-pro/checkpoint-1092, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 05-08 13:01:43 multiproc_worker_utils.py:300] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 05-08 13:01:43 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:01:43 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 05-08 13:02:02 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:02:03 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:02:04 utils.py:950] Found nccl from library libnccl.so.2
INFO 05-08 13:02:04 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:02:04 pynccl.py:69] vLLM is using nccl==2.19.3
INFO 05-08 13:02:04 pynccl.py:69] vLLM is using nccl==2.19.3
INFO 05-08 13:02:05 custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-08 13:05:53 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:05:53 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-08 13:05:53 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6d88c452'), local_subscribe_port=54209, remote_subscribe_port=None)
INFO 05-08 13:05:54 model_runner.py:1110] Starting to load model ./outputs/qwen2.5-3b-sft-pro/checkpoint-1092...
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:05:54 model_runner.py:1110] Starting to load model ./outputs/qwen2.5-3b-sft-pro/checkpoint-1092...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:07<00:07,  7.49s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:37<00:00, 20.65s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:37<00:00, 18.68s/it]

[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:06:32 model_runner.py:1115] Loading model weights took 2.9348 GB
INFO 05-08 13:06:32 model_runner.py:1115] Loading model weights took 2.9348 GB
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:07:36 worker.py:267] Memory profiling takes 64.26 seconds
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:07:36 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:07:36 worker.py:267] model weights take 2.93GiB; non_torch_memory takes 1.57GiB; PyTorch activation peak memory takes 0.53GiB; the rest of the memory reserved for KV Cache is 66.25GiB.
INFO 05-08 13:07:37 worker.py:267] Memory profiling takes 64.71 seconds
INFO 05-08 13:07:37 worker.py:267] the current vLLM instance can use total_gpu_memory (79.21GiB) x gpu_memory_utilization (0.90) = 71.29GiB
INFO 05-08 13:07:37 worker.py:267] model weights take 2.93GiB; non_torch_memory takes 1.82GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 65.09GiB.
INFO 05-08 13:07:37 executor_base.py:110] # CUDA blocks: 236977, # CPU blocks: 14563
INFO 05-08 13:07:37 executor_base.py:115] Maximum concurrency for 10000 tokens per request: 379.16x
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:07:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-08 13:07:40 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:26,  1.30it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:21,  1.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:18,  1.76it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:17,  1.78it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:16,  1.78it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.77it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:15,  1.77it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:15,  1.76it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:14,  1.78it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:13,  1.91it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:11,  2.10it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:10,  2.27it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:06<00:09,  2.41it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:08,  2.40it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:07<00:08,  2.31it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:08,  2.22it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:08<00:08,  2.21it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:07,  2.16it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:09<00:07,  2.13it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.20it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:10<00:06,  2.27it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:10<00:05,  2.34it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:11<00:04,  2.40it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:11<00:04,  2.51it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:03,  2.51it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:12<00:03,  2.49it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.50it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:13<00:02,  2.55it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:13<00:02,  2.52it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:01,  2.56it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:14<00:01,  2.53it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.44it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:15<00:00,  2.39it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.35it/s][1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:08:02 custom_all_reduce.py:226] Registering 2555 cuda graph addresses
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  2.18s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.60it/s]
INFO 05-08 13:08:02 custom_all_reduce.py:226] Registering 2555 cuda graph addresses
[1;36m(VllmWorkerProcess pid=6087)[0;0m INFO 05-08 13:08:02 model_runner.py:1562] Graph capturing finished in 22 secs, took 0.25 GiB
INFO 05-08 13:08:02 model_runner.py:1562] Graph capturing finished in 22 secs, took 0.25 GiB
INFO 05-08 13:08:02 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 89.78 seconds
Processed prompts:   0%|          | 0/2688 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   0%|          | 1/2688 [06:56<311:13:22, 416.97s/it, est. speed input: 0.44 toks/s, output: 1135.67 toks/s]Processed prompts:   0%|          | 2/2688 [06:58<128:47:01, 172.61s/it, est. speed input: 0.81 toks/s, output: 2583.66 toks/s]Processed prompts:   0%|          | 3/2688 [10:36<144:06:29, 193.22s/it, est. speed input: 0.76 toks/s, output: 3152.66 toks/s]Processed prompts:   0%|          | 4/2688 [11:35<104:35:37, 140.29s/it, est. speed input: 0.90 toks/s, output: 3232.68 toks/s]Processed prompts:   0%|          | 5/2688 [14:42<117:12:36, 157.27s/it, est. speed input: 0.94 toks/s, output: 3452.65 toks/s]Processed prompts:   0%|          | 6/2688 [18:13<130:39:27, 175.38s/it, est. speed input: 0.92 toks/s, output: 3416.58 toks/s]Processed prompts:   0%|          | 7/2688 [20:13<117:12:54, 157.39s/it, est. speed input: 1.02 toks/s, output: 3411.27 toks/s]Processed prompts:   0%|          | 8/2688 [21:45<101:36:03, 136.48s/it, est. speed input: 1.54 toks/s, output: 3775.23 toks/s]Processed prompts:   0%|          | 9/2688 [22:51<85:07:18, 114.39s/it, est. speed input: 1.71 toks/s, output: 3746.58 toks/s] Processed prompts:   0%|          | 10/2688 [23:05<62:09:35, 83.56s/it, est. speed input: 1.93 toks/s, output: 3950.80 toks/s]Processed prompts:   0%|          | 11/2688 [27:19<100:52:17, 135.65s/it, est. speed input: 1.79 toks/s, output: 3856.62 toks/s]Processed prompts:   0%|          | 12/2688 [30:02<106:56:32, 143.87s/it, est. speed input: 1.72 toks/s, output: 3928.07 toks/s]Processed prompts:   0%|          | 13/2688 [33:07<116:20:28, 156.57s/it, est. speed input: 1.63 toks/s, output: 3799.27 toks/s]Processed prompts:   1%|          | 14/2688 [34:51<104:21:49, 140.50s/it, est. speed input: 1.62 toks/s, output: 3836.27 toks/s]Processed prompts:   1%|          | 15/2688 [37:19<106:02:57, 142.83s/it, est. speed input: 1.65 toks/s, output: 3934.54 toks/s]Processed prompts:   1%|          | 16/2688 [39:48<107:19:34, 144.60s/it, est. speed input: 1.70 toks/s, output: 4066.14 toks/s]Processed prompts:   1%|          | 17/2688 [43:07<119:23:09, 160.91s/it, est. speed input: 1.62 toks/s, output: 3967.21 toks/s]Processed prompts:   1%|          | 18/2688 [46:07<123:44:20, 166.84s/it, est. speed input: 1.59 toks/s, output: 3971.45 toks/s]Processed prompts:   1%|          | 19/2688 [47:56<110:49:44, 149.49s/it, est. speed input: 1.65 toks/s, output: 4041.30 toks/s]Processed prompts:   1%|          | 20/2688 [49:39<100:16:45, 135.31s/it, est. speed input: 1.66 toks/s, output: 4186.89 toks/s]Processed prompts:   1%|          | 21/2688 [50:42<84:19:20, 113.82s/it, est. speed input: 1.67 toks/s, output: 4333.68 toks/s] Processed prompts:   1%|          | 21/2688 [50:42<107:20:36, 144.90s/it, est. speed input: 1.67 toks/s, output: 4333.68 toks/s]
✓ Saved batch 0-20 (2688 rows)

✅ Completed all 128 trials for OOD_BASE
INFO 05-08 13:59:14 multiproc_worker_utils.py:128] Killing local vLLM worker processes
[rank0]:[W508 13:59:35.837177340 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/home/VRL/env/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
